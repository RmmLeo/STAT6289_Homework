{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer,TextClassificationPipeline\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertConfig, AutoModelForTokenClassification\n",
    "\n",
    "# retreive the saved model \n",
    "model = DistilBertForSequenceClassification.from_pretrained('C:\\RMM\\Medical3\\distilbert-base-uncased-finetuned-sst-2-english', \n",
    "                                                        local_files_only=True)\n",
    "\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "tokenizer = transformers.DistilBertTokenizerFast.from_pretrained(tokenizer_name)\n",
    "pipe=TextClassificationPipeline(tokenizer=tokenizer,model=model,device=0,return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_x = np.load(\"test2.npy\",allow_pickle=True)\n",
    "cor_x=cor_x[:200]\n",
    "cor_reviews = [review[0] for review in cor_x]\n",
    "cor_labels = [review[1] for review in cor_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_top_k(k, pred_label_no_mask, values, returned_tokens):\n",
    "    \"\"\"\n",
    "    masked the k tokens that have the max shap values\n",
    "    :param k: specify the largest k value\n",
    "    :param values: shap values\n",
    "    :param returned_tokens: a list of tokens\n",
    "    :return: review, which is a str constructed from a list words\n",
    "    \"\"\"\n",
    "    shap_values_neg, shap_values_pos = zip(*values)\n",
    "    values = shap_values_neg if pred_label_no_mask==0 else shap_values_pos\n",
    "    # print(values)\n",
    "    values = np.array(values)\n",
    "    # ids_top_k = np.argpartition(values, -k)[-k:]\n",
    "    ids_top_k = (-values).argsort()[:k]\n",
    "    for idx in ids_top_k:\n",
    "        # print(idx)\n",
    "        returned_tokens[idx] = \"[UNK] \"\n",
    "    masked_review = \"\".join(returned_tokens)\n",
    "    # print(masked_review)\n",
    "    return masked_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(pipe, masked_review):\n",
    "    \"\"\"\n",
    "    predict the label for the masked_review\n",
    "    :param pipe: pipeline\n",
    "    :param masked_review: string\n",
    "    :return: 0 or 1, indicating the label\n",
    "    \"\"\"\n",
    "    prediction = pipe([masked_review])\n",
    "    if prediction[0]['label'] == 'NEGATIVE':\n",
    "            neg_score = prediction[0][\"score\"]\n",
    "            pos_score=1-prediction[0][\"score\"]\n",
    "    else:\n",
    "            pos_score = prediction[0][\"score\"] \n",
    "            neg_score = 1-prediction[0][\"score\"]       \n",
    "    pred_label = 0 if prediction[0]['label'] == 'NEGATIVE'  else 1\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_top_k(k, pred_label_no_mask, values, returned_tokens):\n",
    "    \"\"\"\n",
    "    masked the k tokens that have the max shap values\n",
    "    :param k: specify the largest k value\n",
    "    :param values: shap values\n",
    "    :param returned_tokens: a list of tokens\n",
    "    :return: review, which is a str constructed from a list words\n",
    "    \"\"\"\n",
    "    shap_values_neg, shap_values_pos = zip(*values)\n",
    "    values = shap_values_neg if pred_label_no_mask==0 else shap_values_pos\n",
    "    # print(values)\n",
    "    values = np.array(values)\n",
    "    # ids_top_k = np.argpartition(values, -k)[-k:]\n",
    "    ids_top_k = (-values).argsort()[:k]\n",
    "    for idx in ids_top_k:\n",
    "        # print(idx)\n",
    "        returned_tokens[idx] = \"[UNK] \"\n",
    "    masked_review = \"\".join(returned_tokens)\n",
    "    # print(masked_review)\n",
    "    return masked_review\n",
    "\n",
    "def predict_label(pipe, masked_review):\n",
    "    \"\"\"\n",
    "    predict the label for the masked_review\n",
    "    :param pipe: pipeline\n",
    "    :param masked_review: string\n",
    "    :return: 0 or 1, indicating the label\n",
    "    \"\"\"\n",
    "    prediction = pipe([masked_review])\n",
    "    neg_prediction_score = prediction[0][0][\"score\"]\n",
    "    pos_prediction_score = prediction[0][1][\"score\"]\n",
    "    pred_label = 0 if neg_prediction_score > pos_prediction_score else 1\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_gpu(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', \n",
    "                                        max_length=80, truncation=True) for v in x]).cuda()\n",
    "    attention_mask = (tv!=0).type(torch.int64).cuda()\n",
    "    outputs = model(tv, attention_mask=attention_mask)[0]\n",
    "    scores = torch.nn.Softmax(dim=-1)(outputs)\n",
    "    val = torch.logit(scores).detach().cpu().numpy()\n",
    "\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process 0-th review\n",
      "process 1-th review\n",
      "process 2-th review\n",
      "process 3-th review\n",
      "process 4-th review\n",
      "process 5-th review\n",
      "process 6-th review\n",
      "process 7-th review\n",
      "process 8-th review\n",
      "process 9-th review\n",
      "process 10-th review\n",
      "process 11-th review\n",
      "process 12-th review\n",
      "process 13-th review\n",
      "process 14-th review\n",
      "process 15-th review\n",
      "process 16-th review\n",
      "process 17-th review\n",
      "process 18-th review\n",
      "process 19-th review\n",
      "process 20-th review\n",
      "process 21-th review\n",
      "process 22-th review\n",
      "process 23-th review\n",
      "process 24-th review\n",
      "process 25-th review\n",
      "process 26-th review\n",
      "process 27-th review\n",
      "process 28-th review\n",
      "process 29-th review\n",
      "process 30-th review\n",
      "process 31-th review\n",
      "process 32-th review\n",
      "process 33-th review\n",
      "process 34-th review\n",
      "process 35-th review\n",
      "process 36-th review\n",
      "process 37-th review\n",
      "process 38-th review\n",
      "process 39-th review\n",
      "process 40-th review\n",
      "process 41-th review\n",
      "process 42-th review\n",
      "process 43-th review\n",
      "process 44-th review\n",
      "process 45-th review\n",
      "process 46-th review\n",
      "process 47-th review\n",
      "process 48-th review\n",
      "process 49-th review\n",
      "process 50-th review\n",
      "process 51-th review\n",
      "process 52-th review\n",
      "process 53-th review\n",
      "process 54-th review\n",
      "process 55-th review\n",
      "process 56-th review\n",
      "process 57-th review\n",
      "process 58-th review\n",
      "process 59-th review\n",
      "process 60-th review\n",
      "process 61-th review\n",
      "process 62-th review\n",
      "process 63-th review\n",
      "process 64-th review\n",
      "process 65-th review\n",
      "process 66-th review\n",
      "process 67-th review\n",
      "process 68-th review\n",
      "process 69-th review\n",
      "process 70-th review\n",
      "process 71-th review\n",
      "process 72-th review\n",
      "process 73-th review\n",
      "process 74-th review\n",
      "process 75-th review\n",
      "process 76-th review\n",
      "process 77-th review\n",
      "process 78-th review\n",
      "process 79-th review\n",
      "process 80-th review\n",
      "process 81-th review\n",
      "process 82-th review\n",
      "process 83-th review\n",
      "process 84-th review\n",
      "process 85-th review\n",
      "process 86-th review\n",
      "process 87-th review\n",
      "process 88-th review\n",
      "process 89-th review\n",
      "process 90-th review\n",
      "process 91-th review\n",
      "process 92-th review\n",
      "process 93-th review\n",
      "process 94-th review\n",
      "process 95-th review\n",
      "process 96-th review\n",
      "process 97-th review\n",
      "process 98-th review\n",
      "process 99-th review\n",
      "process 100-th review\n",
      "process 101-th review\n",
      "process 102-th review\n",
      "process 103-th review\n",
      "process 104-th review\n",
      "process 105-th review\n",
      "process 106-th review\n",
      "process 107-th review\n",
      "process 108-th review\n",
      "process 109-th review\n",
      "process 110-th review\n",
      "process 111-th review\n",
      "process 112-th review\n",
      "process 113-th review\n",
      "process 114-th review\n",
      "process 115-th review\n",
      "process 116-th review\n",
      "process 117-th review\n",
      "process 118-th review\n",
      "process 119-th review\n",
      "process 120-th review\n",
      "process 121-th review\n",
      "process 122-th review\n",
      "process 123-th review\n",
      "process 124-th review\n",
      "process 125-th review\n",
      "process 126-th review\n",
      "process 127-th review\n",
      "process 128-th review\n",
      "process 129-th review\n",
      "process 130-th review\n",
      "process 131-th review\n",
      "process 132-th review\n",
      "process 133-th review\n",
      "process 134-th review\n",
      "process 135-th review\n",
      "process 136-th review\n",
      "process 137-th review\n",
      "process 138-th review\n",
      "process 139-th review\n",
      "process 140-th review\n",
      "process 141-th review\n",
      "process 142-th review\n",
      "process 143-th review\n",
      "process 144-th review\n",
      "process 145-th review\n",
      "process 146-th review\n",
      "process 147-th review\n",
      "process 148-th review\n",
      "process 149-th review\n",
      "process 150-th review\n",
      "process 151-th review\n",
      "process 152-th review\n",
      "process 153-th review\n",
      "process 154-th review\n",
      "process 155-th review\n",
      "process 156-th review\n",
      "process 157-th review\n",
      "process 158-th review\n",
      "process 159-th review\n",
      "process 160-th review\n",
      "process 161-th review\n",
      "process 162-th review\n",
      "process 163-th review\n",
      "process 164-th review\n",
      "process 165-th review\n",
      "process 166-th review\n",
      "process 167-th review\n",
      "process 168-th review\n",
      "process 169-th review\n",
      "process 170-th review\n",
      "process 171-th review\n",
      "process 172-th review\n",
      "process 173-th review\n",
      "process 174-th review\n",
      "process 175-th review\n",
      "process 176-th review\n",
      "process 177-th review\n",
      "process 178-th review\n",
      "process 179-th review\n",
      "process 180-th review\n",
      "process 181-th review\n",
      "process 182-th review\n",
      "process 183-th review\n",
      "process 184-th review\n",
      "process 185-th review\n",
      "process 186-th review\n",
      "process 187-th review\n",
      "process 188-th review\n",
      "process 189-th review\n",
      "process 190-th review\n",
      "process 191-th review\n",
      "process 192-th review\n",
      "process 193-th review\n",
      "process 194-th review\n",
      "process 195-th review\n",
      "process 196-th review\n",
      "process 197-th review\n",
      "process 198-th review\n",
      "process 199-th review\n"
     ]
    }
   ],
   "source": [
    "shap_values_list = []\n",
    "token_data_list = []\n",
    "top_k = [1, 3, 5, 7,9,11,13]\n",
    "all_labels =[]\n",
    "# use GPU\n",
    "gpu_explainer = shap.Explainer(model_prediction_gpu, tokenizer)\n",
    "i = 0\n",
    "for review, label in zip(cor_reviews, cor_labels):\n",
    "    print(f\"process {i}-th review\")\n",
    "    i += 1\n",
    "    label4review =[]\n",
    "    label4review.append(label)\n",
    "    # to-do: truncate review if len(review)>80\n",
    "    tokens = tokenizer.tokenize(review)\n",
    "    if len(tokens) > 80:\n",
    "        tokens_truncated = tokens[:80]\n",
    "        review = \" \".join(token for token in tokens_truncated)\n",
    "    pred_label_no_mask = predict_label(pipe, review) # predicted label for review without mask\n",
    "    label4review.append(pred_label_no_mask)\n",
    "    shap_values = gpu_explainer([review])\n",
    "    values = shap_values.values[0] # 2-dim ndarray\n",
    "    returned_tokens = shap_values.data[0]\n",
    "    for k in top_k:\n",
    "        masked_review = mask_top_k(k, pred_label_no_mask, values, returned_tokens) # mask review by the shap values\n",
    "        predicted_label= predict_label(pipe, masked_review)\n",
    "        label4review.append(predicted_label)\n",
    "    # label4review = [True_label, pred_label_without_mask, masked_label_1, masked_label_2, masked_review_3, masked_review_4]\n",
    "    all_labels.append(label4review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(np.array(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df.loc[df[0]==df[1]]\n",
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778 0.46405228758169936 0.3464052287581699 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(filtered_df[1], filtered_df[2]), accuracy_score(filtered_df[1], filtered_df[3]), accuracy_score(filtered_df[1], filtered_df[4]), accuracy_score(filtered_df[1], filtered_df[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_masked=[accuracy_score(filtered_df[1], filtered_df[2]), accuracy_score(filtered_df[1], filtered_df[3]), accuracy_score(filtered_df[1], filtered_df[4]), accuracy_score(filtered_df[1], filtered_df[5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7777777777777778,\n",
       " 0.46405228758169936,\n",
       " 0.3464052287581699,\n",
       " 0.3333333333333333]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7777777777777778 0.5816993464052288 0.46405228758169936 0.3660130718954248 0.3464052287581699 0.35947712418300654 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(filtered_df[1], filtered_df[2]), accuracy_score(filtered_df[1], filtered_df[3]), accuracy_score(filtered_df[1], filtered_df[4]), accuracy_score(filtered_df[1], filtered_df[5]),accuracy_score(filtered_df[1], filtered_df[6]),accuracy_score(filtered_df[1], filtered_df[7]),accuracy_score(filtered_df[1], filtered_df[8]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aa083d928abac1bb4c2de21863d23e101a74b57ce6d7b6a3a0a67f44f67e603"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('Leo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
